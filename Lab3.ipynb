{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 3\n",
    "\n",
    "#### Mario de Le√≥n\n",
    "#### Javier Valle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Feed Forward NN (Red simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 7s 16ms/step - loss: 4546.4067 - val_loss: 4338.5601\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 3855.0554 - val_loss: 1811.6208\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1819.1913 - val_loss: 1787.5450\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1785.5952 - val_loss: 1763.0090\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1771.0082 - val_loss: 1759.8112\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1769.8899 - val_loss: 1750.1511\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1763.1667 - val_loss: 1751.3389\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1768.5315 - val_loss: 1747.0206\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1767.5894 - val_loss: 1742.8634\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1758.3241 - val_loss: 1757.1016\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1754.4252 - val_loss: 1749.4121\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1758.3289 - val_loss: 1741.6356\n",
      "Epoch 13/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1762.0895 - val_loss: 1740.4441\n",
      "Epoch 14/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1766.7117 - val_loss: 1749.8030\n",
      "Epoch 15/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1753.6582 - val_loss: 1740.5582\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1751.5139 - val_loss: 1759.9443\n",
      "Epoch 17/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1753.2957 - val_loss: 1748.8143\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1748.3348 - val_loss: 1744.2581\n",
      "Epoch 19/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1752.5054 - val_loss: 1753.4377\n",
      "Epoch 20/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1749.8604 - val_loss: 1740.3528\n",
      "Epoch 21/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1747.4036 - val_loss: 1753.5547\n",
      "Epoch 22/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1747.0027 - val_loss: 1768.4253\n",
      "Epoch 23/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1741.9644 - val_loss: 1768.5696\n",
      "Epoch 24/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1741.2930 - val_loss: 1738.4758\n",
      "Epoch 25/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1742.7115 - val_loss: 1736.8845\n",
      "Epoch 26/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1736.5798 - val_loss: 1748.2021\n",
      "Epoch 27/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1736.4030 - val_loss: 1742.4565\n",
      "Epoch 28/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1741.0013 - val_loss: 1748.1190\n",
      "Epoch 29/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1738.0142 - val_loss: 1730.6940\n",
      "Epoch 30/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1732.8578 - val_loss: 1729.6436\n",
      "Epoch 31/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1729.7916 - val_loss: 1743.6990\n",
      "Epoch 32/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1739.8422 - val_loss: 1760.2527\n",
      "Epoch 33/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1729.5720 - val_loss: 1739.6726\n",
      "Epoch 34/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1719.4731 - val_loss: 1767.1858\n",
      "Epoch 35/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1719.8734 - val_loss: 1721.5258\n",
      "Epoch 36/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1712.3088 - val_loss: 1727.9285\n",
      "Epoch 37/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1704.5298 - val_loss: 1741.3143\n",
      "Epoch 38/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1710.3129 - val_loss: 1721.2329\n",
      "Epoch 39/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1700.2699 - val_loss: 1732.1417\n",
      "Epoch 40/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1719.4069 - val_loss: 1724.9449\n",
      "Epoch 41/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1704.6821 - val_loss: 1718.8486\n",
      "Epoch 42/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1696.5796 - val_loss: 1721.0244\n",
      "Epoch 43/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1692.0137 - val_loss: 1719.8783\n",
      "Epoch 44/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1694.0233 - val_loss: 1722.2705\n",
      "Epoch 45/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1683.5782 - val_loss: 1737.2565\n",
      "Epoch 46/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1695.5751 - val_loss: 1727.8037\n",
      "Epoch 47/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1685.7914 - val_loss: 1723.4542\n",
      "Epoch 48/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1678.4440 - val_loss: 1733.9769\n",
      "Epoch 49/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1679.7939 - val_loss: 1731.3884\n",
      "Epoch 50/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1679.4584 - val_loss: 1731.8179\n",
      "Epoch 51/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1671.9718 - val_loss: 1729.5282\n",
      "Epoch 52/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1692.3569 - val_loss: 1794.1947\n",
      "Epoch 53/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1691.1890 - val_loss: 1724.8867\n",
      "Epoch 54/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1680.4745 - val_loss: 1737.1731\n",
      "Epoch 55/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1687.5078 - val_loss: 1722.8816\n",
      "Epoch 56/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1671.9128 - val_loss: 1725.3923\n",
      "Epoch 57/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1670.4989 - val_loss: 1727.8301\n",
      "Epoch 58/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1667.9885 - val_loss: 1732.3022\n",
      "Epoch 59/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1663.6482 - val_loss: 1723.4095\n",
      "Epoch 60/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1663.4709 - val_loss: 1772.2476\n",
      "Epoch 61/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1662.0253 - val_loss: 1737.2246\n",
      "Epoch 62/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1670.1370 - val_loss: 1735.3894\n",
      "Epoch 63/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1685.6768 - val_loss: 1726.2655\n",
      "Epoch 64/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1661.9347 - val_loss: 1739.6655\n",
      "Epoch 65/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1669.2646 - val_loss: 1739.7756\n",
      "Epoch 66/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1662.2305 - val_loss: 1782.2551\n",
      "Epoch 67/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1662.8359 - val_loss: 1744.6016\n",
      "Epoch 68/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1665.5558 - val_loss: 1732.5122\n",
      "Epoch 69/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1662.0636 - val_loss: 1725.8634\n",
      "Epoch 70/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1665.6161 - val_loss: 1788.8560\n",
      "Epoch 71/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1660.4807 - val_loss: 1731.1935\n",
      "Epoch 72/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1653.0526 - val_loss: 1756.8575\n",
      "Epoch 73/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1663.1628 - val_loss: 1771.9272\n",
      "Epoch 74/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1661.9312 - val_loss: 1741.9452\n",
      "Epoch 75/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1664.0955 - val_loss: 1734.1909\n",
      "Epoch 76/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1649.0504 - val_loss: 1732.1987\n",
      "Epoch 77/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1657.2443 - val_loss: 1734.4310\n",
      "Epoch 78/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1669.2648 - val_loss: 1725.2771\n",
      "Epoch 79/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1665.6875 - val_loss: 1755.3733\n",
      "Epoch 80/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1661.0245 - val_loss: 1741.5736\n",
      "Epoch 81/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1656.8569 - val_loss: 1728.5535\n",
      "Epoch 82/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1651.5499 - val_loss: 1731.2710\n",
      "Epoch 83/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1648.1066 - val_loss: 1741.2950\n",
      "Epoch 84/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1650.5289 - val_loss: 1778.6510\n",
      "Epoch 85/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1654.3413 - val_loss: 1739.9731\n",
      "Epoch 86/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1670.1752 - val_loss: 1790.5422\n",
      "Epoch 87/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1657.2056 - val_loss: 1731.4513\n",
      "Epoch 88/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1669.4460 - val_loss: 1734.3047\n",
      "Epoch 89/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1658.6520 - val_loss: 1770.9568\n",
      "Epoch 90/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1650.4891 - val_loss: 1810.3746\n",
      "Epoch 91/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1656.9343 - val_loss: 1782.6886\n",
      "Epoch 92/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1645.5992 - val_loss: 1729.2013\n",
      "Epoch 93/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1657.8156 - val_loss: 1732.9543\n",
      "Epoch 94/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1656.6281 - val_loss: 1730.7761\n",
      "Epoch 95/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1646.5115 - val_loss: 1731.7356\n",
      "Epoch 96/100\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 1649.2570 - val_loss: 1770.7186\n",
      "Epoch 97/100\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 1650.7886 - val_loss: 1733.5022\n",
      "Epoch 98/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1662.9893 - val_loss: 1829.9994\n",
      "Epoch 99/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1659.9906 - val_loss: 1737.4656\n",
      "Epoch 100/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1649.0422 - val_loss: 1736.4835\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1736.4835\n",
      "P√©rdida en el conjunto de prueba: 1736.4835205078125\n",
      "18/18 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset\n",
    "data = pd.read_csv('data.csv') \n",
    "\n",
    "\n",
    "# Separando un poco el formato.\n",
    "data['Year'] = data['Month'].apply(lambda x: int(x.split('-')[0]))\n",
    "data['Month'] = data['Month'].apply(lambda x: int(x.split('-')[1]))\n",
    "\n",
    "# Dividir los datos en caracter√≠sticas (X) y etiquetas (y)\n",
    "X = data[['Year', 'Month']]\n",
    "y = data['Sunspots']\n",
    "\n",
    "# Conjunto de entrenamiento y prueba.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalando los datos.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creando la estructura.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,)),  # 2 caracter√≠sticas de entrada (Year, Month)\n",
    "    tf.keras.layers.Dense(256, activation='relu'),  \n",
    "    tf.keras.layers.Dense(128, activation='relu'),  \n",
    "    tf.keras.layers.Dense(64, activation='relu'),  \n",
    "    tf.keras.layers.Dense(32, activation='relu'),  \n",
    "    tf.keras.layers.Dense(16, activation='relu'),  \n",
    "    tf.keras.layers.Dense(8, activation='relu'),  \n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Capa de salida para la regresi√≥n\n",
    "])\n",
    "\n",
    "# Compilando el modelo.\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenando el modelo\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluando el modelo.\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"P√©rdida en el conjunto de prueba:\", loss)\n",
    "\n",
    "# Realiznado predicciones.\n",
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicciones del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones:  [[55.227173]\n",
      " [60.872795]\n",
      " [34.035294]\n",
      " [56.249866]\n",
      " [88.17943 ]\n",
      " [53.79607 ]\n",
      " [41.36508 ]\n",
      " [95.34134 ]\n",
      " [56.009155]\n",
      " [31.893251]\n",
      " [55.393543]\n",
      " [42.97194 ]\n",
      " [46.98934 ]\n",
      " [47.54872 ]\n",
      " [33.69964 ]\n",
      " [56.51853 ]\n",
      " [88.1566  ]\n",
      " [66.36552 ]\n",
      " [56.201347]\n",
      " [51.372337]\n",
      " [81.52766 ]\n",
      " [40.858154]\n",
      " [51.681473]\n",
      " [36.1304  ]\n",
      " [36.607994]\n",
      " [53.161255]\n",
      " [35.931458]\n",
      " [61.28434 ]\n",
      " [58.846924]\n",
      " [34.040432]\n",
      " [61.04664 ]\n",
      " [40.87494 ]\n",
      " [52.56299 ]\n",
      " [31.881088]\n",
      " [43.788307]\n",
      " [34.323017]\n",
      " [42.75274 ]\n",
      " [52.868546]\n",
      " [57.604073]\n",
      " [40.716267]\n",
      " [62.66231 ]\n",
      " [56.216   ]\n",
      " [78.69503 ]\n",
      " [60.855335]\n",
      " [45.55175 ]\n",
      " [57.458702]\n",
      " [51.331886]\n",
      " [29.193861]\n",
      " [30.524961]\n",
      " [60.38593 ]\n",
      " [44.451607]\n",
      " [43.455833]\n",
      " [61.30581 ]\n",
      " [59.612896]\n",
      " [84.8598  ]\n",
      " [34.39939 ]\n",
      " [32.09374 ]\n",
      " [38.761   ]\n",
      " [94.49737 ]\n",
      " [66.64712 ]\n",
      " [58.48913 ]\n",
      " [52.962143]\n",
      " [62.766556]\n",
      " [46.027298]\n",
      " [89.45785 ]\n",
      " [54.77449 ]\n",
      " [91.98978 ]\n",
      " [71.88612 ]\n",
      " [34.303387]\n",
      " [50.970036]\n",
      " [44.799126]\n",
      " [36.65245 ]\n",
      " [45.261303]\n",
      " [40.30471 ]\n",
      " [43.9731  ]\n",
      " [38.10045 ]\n",
      " [32.137432]\n",
      " [36.07618 ]\n",
      " [73.139175]\n",
      " [34.22541 ]\n",
      " [34.57752 ]\n",
      " [65.8241  ]\n",
      " [90.54739 ]\n",
      " [60.652817]\n",
      " [51.113255]\n",
      " [58.97464 ]\n",
      " [52.969864]\n",
      " [73.89906 ]\n",
      " [50.262917]\n",
      " [92.81276 ]\n",
      " [48.054474]\n",
      " [35.42929 ]\n",
      " [40.848873]\n",
      " [50.360107]\n",
      " [41.96946 ]\n",
      " [34.813362]\n",
      " [69.06472 ]\n",
      " [51.725014]\n",
      " [42.35731 ]\n",
      " [35.789467]\n",
      " [32.964645]\n",
      " [54.6586  ]\n",
      " [38.670975]\n",
      " [48.78927 ]\n",
      " [58.083458]\n",
      " [58.00116 ]\n",
      " [46.906395]\n",
      " [40.410385]\n",
      " [83.09349 ]\n",
      " [56.588646]\n",
      " [50.036583]\n",
      " [84.09346 ]\n",
      " [52.445946]\n",
      " [42.027386]\n",
      " [59.8002  ]\n",
      " [36.43341 ]\n",
      " [86.02634 ]\n",
      " [35.116768]\n",
      " [84.394   ]\n",
      " [59.732758]\n",
      " [34.657967]\n",
      " [37.194084]\n",
      " [47.99815 ]\n",
      " [48.266426]\n",
      " [37.268997]\n",
      " [37.557922]\n",
      " [44.08766 ]\n",
      " [90.30182 ]\n",
      " [62.025497]\n",
      " [54.110397]\n",
      " [34.440453]\n",
      " [83.70703 ]\n",
      " [51.452785]\n",
      " [53.108612]\n",
      " [52.86943 ]\n",
      " [53.568817]\n",
      " [36.764103]\n",
      " [41.293503]\n",
      " [32.35666 ]\n",
      " [54.691185]\n",
      " [56.484833]\n",
      " [56.848175]\n",
      " [30.270006]\n",
      " [36.18167 ]\n",
      " [60.72235 ]\n",
      " [56.598015]\n",
      " [58.749535]\n",
      " [56.41562 ]\n",
      " [74.26607 ]\n",
      " [59.028793]\n",
      " [57.882267]\n",
      " [81.69114 ]\n",
      " [60.976242]\n",
      " [41.235123]\n",
      " [45.42299 ]\n",
      " [61.191704]\n",
      " [56.048073]\n",
      " [55.138817]\n",
      " [42.220375]\n",
      " [68.30939 ]\n",
      " [91.965546]\n",
      " [53.429092]\n",
      " [52.29694 ]\n",
      " [49.572666]\n",
      " [58.16124 ]\n",
      " [67.08055 ]\n",
      " [58.23653 ]\n",
      " [78.46239 ]\n",
      " [37.55282 ]\n",
      " [36.242447]\n",
      " [32.00414 ]\n",
      " [31.46565 ]\n",
      " [39.98089 ]\n",
      " [60.246815]\n",
      " [48.548607]\n",
      " [45.51011 ]\n",
      " [52.814346]\n",
      " [34.967957]\n",
      " [38.3861  ]\n",
      " [34.679405]\n",
      " [36.408127]\n",
      " [57.537445]\n",
      " [46.904305]\n",
      " [59.090523]\n",
      " [40.26758 ]\n",
      " [55.326805]\n",
      " [42.05227 ]\n",
      " [44.37291 ]\n",
      " [51.74481 ]\n",
      " [47.106503]\n",
      " [39.88486 ]\n",
      " [36.950745]\n",
      " [47.31244 ]\n",
      " [40.78795 ]\n",
      " [47.12439 ]\n",
      " [36.79986 ]\n",
      " [59.885635]\n",
      " [48.004234]\n",
      " [75.28313 ]\n",
      " [38.409676]\n",
      " [59.418976]\n",
      " [66.50473 ]\n",
      " [61.39209 ]\n",
      " [81.862076]\n",
      " [57.231693]\n",
      " [53.69388 ]\n",
      " [53.35399 ]\n",
      " [39.30113 ]\n",
      " [47.555134]\n",
      " [34.765953]\n",
      " [40.877804]\n",
      " [34.527622]\n",
      " [51.555084]\n",
      " [43.286896]\n",
      " [75.652954]\n",
      " [38.27854 ]\n",
      " [60.31795 ]\n",
      " [51.842953]\n",
      " [48.85427 ]\n",
      " [36.720013]\n",
      " [33.757812]\n",
      " [59.8408  ]\n",
      " [44.459038]\n",
      " [43.810707]\n",
      " [46.210266]\n",
      " [71.00683 ]\n",
      " [37.952602]\n",
      " [44.43787 ]\n",
      " [57.300877]\n",
      " [60.830933]\n",
      " [77.66188 ]\n",
      " [44.869926]\n",
      " [63.797752]\n",
      " [62.86081 ]\n",
      " [35.574085]\n",
      " [62.053608]\n",
      " [50.763275]\n",
      " [43.08407 ]\n",
      " [42.41638 ]\n",
      " [55.525402]\n",
      " [35.07914 ]\n",
      " [36.09903 ]\n",
      " [40.62102 ]\n",
      " [47.168457]\n",
      " [45.75125 ]\n",
      " [32.0944  ]\n",
      " [37.90616 ]\n",
      " [46.76355 ]\n",
      " [73.589066]\n",
      " [38.85759 ]\n",
      " [50.073914]\n",
      " [46.611946]\n",
      " [82.70604 ]\n",
      " [62.242706]\n",
      " [66.60068 ]\n",
      " [39.741028]\n",
      " [37.19311 ]\n",
      " [88.61386 ]\n",
      " [43.335457]\n",
      " [41.092037]\n",
      " [54.08138 ]\n",
      " [31.447828]\n",
      " [52.417107]\n",
      " [75.3629  ]\n",
      " [68.93132 ]\n",
      " [63.70476 ]\n",
      " [35.187035]\n",
      " [42.667023]\n",
      " [93.21999 ]\n",
      " [64.89723 ]\n",
      " [58.72027 ]\n",
      " [34.99267 ]\n",
      " [57.217125]\n",
      " [49.56598 ]\n",
      " [59.485023]\n",
      " [45.396317]\n",
      " [35.78654 ]\n",
      " [30.140055]\n",
      " [49.283134]\n",
      " [37.238716]\n",
      " [32.701836]\n",
      " [37.507122]\n",
      " [47.79638 ]\n",
      " [50.207443]\n",
      " [59.27494 ]\n",
      " [60.93722 ]\n",
      " [33.569435]\n",
      " [30.988333]\n",
      " [52.972923]\n",
      " [33.181175]\n",
      " [42.31583 ]\n",
      " [55.37664 ]\n",
      " [52.46737 ]\n",
      " [42.805374]\n",
      " [59.78526 ]\n",
      " [56.060123]\n",
      " [41.58612 ]\n",
      " [85.40669 ]\n",
      " [51.221153]\n",
      " [50.055473]\n",
      " [46.87359 ]\n",
      " [41.165977]\n",
      " [61.752052]\n",
      " [62.34208 ]\n",
      " [55.091797]\n",
      " [61.7334  ]\n",
      " [55.23153 ]\n",
      " [48.004936]\n",
      " [56.281082]\n",
      " [47.217026]\n",
      " [58.214928]\n",
      " [34.738983]\n",
      " [36.817116]\n",
      " [53.648724]\n",
      " [39.94825 ]\n",
      " [59.529587]\n",
      " [49.10534 ]\n",
      " [50.00521 ]\n",
      " [34.714592]\n",
      " [39.22894 ]\n",
      " [46.20932 ]\n",
      " [63.399475]\n",
      " [34.83496 ]\n",
      " [37.91282 ]\n",
      " [52.81826 ]\n",
      " [34.90395 ]\n",
      " [54.024284]\n",
      " [38.162388]\n",
      " [54.425507]\n",
      " [48.363804]\n",
      " [31.71599 ]\n",
      " [38.625835]\n",
      " [59.129498]\n",
      " [40.403984]\n",
      " [74.67609 ]\n",
      " [65.75039 ]\n",
      " [47.75881 ]\n",
      " [56.59311 ]\n",
      " [40.7835  ]\n",
      " [52.5382  ]\n",
      " [51.626457]\n",
      " [46.40605 ]\n",
      " [76.79822 ]\n",
      " [62.499733]\n",
      " [99.654945]\n",
      " [43.262863]\n",
      " [44.42209 ]\n",
      " [37.74109 ]\n",
      " [72.56267 ]\n",
      " [51.61486 ]\n",
      " [36.077984]\n",
      " [41.563168]\n",
      " [89.75828 ]\n",
      " [60.686058]\n",
      " [74.714676]\n",
      " [49.7168  ]\n",
      " [41.119797]\n",
      " [88.72556 ]\n",
      " [91.12981 ]\n",
      " [50.5214  ]\n",
      " [42.999992]\n",
      " [74.7648  ]\n",
      " [53.10432 ]\n",
      " [49.36174 ]\n",
      " [68.30409 ]\n",
      " [89.69905 ]\n",
      " [59.768078]\n",
      " [35.563675]\n",
      " [54.48402 ]\n",
      " [35.93657 ]\n",
      " [44.688637]\n",
      " [42.96518 ]\n",
      " [48.83899 ]\n",
      " [65.95903 ]\n",
      " [40.49649 ]\n",
      " [49.818024]\n",
      " [51.75069 ]\n",
      " [85.23796 ]\n",
      " [56.159046]\n",
      " [60.665405]\n",
      " [38.910652]\n",
      " [43.2804  ]\n",
      " [59.279205]\n",
      " [31.27152 ]\n",
      " [53.653038]\n",
      " [34.077385]\n",
      " [31.126759]\n",
      " [48.004704]\n",
      " [41.50962 ]\n",
      " [52.30962 ]\n",
      " [88.58977 ]\n",
      " [36.626842]\n",
      " [44.34127 ]\n",
      " [38.4218  ]\n",
      " [35.96632 ]\n",
      " [52.448727]\n",
      " [55.764427]\n",
      " [61.70355 ]\n",
      " [51.523914]\n",
      " [44.632126]\n",
      " [50.504074]\n",
      " [91.15796 ]\n",
      " [58.757427]\n",
      " [49.62715 ]\n",
      " [88.00575 ]\n",
      " [45.595062]\n",
      " [73.43001 ]\n",
      " [60.384644]\n",
      " [40.85185 ]\n",
      " [45.76928 ]\n",
      " [54.515877]\n",
      " [58.616753]\n",
      " [52.511833]\n",
      " [47.6777  ]\n",
      " [52.43708 ]\n",
      " [58.26786 ]\n",
      " [55.301468]\n",
      " [42.43805 ]\n",
      " [46.437042]\n",
      " [30.686964]\n",
      " [41.17324 ]\n",
      " [37.169834]\n",
      " [52.834198]\n",
      " [61.18376 ]\n",
      " [44.183887]\n",
      " [59.460217]\n",
      " [43.162117]\n",
      " [69.831055]\n",
      " [68.08449 ]\n",
      " [93.22409 ]\n",
      " [97.33429 ]\n",
      " [31.512968]\n",
      " [61.05963 ]\n",
      " [58.21678 ]\n",
      " [37.682625]\n",
      " [38.338158]\n",
      " [38.00396 ]\n",
      " [34.30812 ]\n",
      " [41.304626]\n",
      " [58.641087]\n",
      " [64.29921 ]\n",
      " [35.862083]\n",
      " [43.892303]\n",
      " [43.72397 ]\n",
      " [54.829887]\n",
      " [51.331802]\n",
      " [59.027023]\n",
      " [53.350136]\n",
      " [55.27207 ]\n",
      " [57.242203]\n",
      " [42.809837]\n",
      " [58.160393]\n",
      " [39.64765 ]\n",
      " [83.19373 ]\n",
      " [43.9934  ]\n",
      " [46.163918]\n",
      " [42.273125]\n",
      " [93.0925  ]\n",
      " [43.937767]\n",
      " [81.01811 ]\n",
      " [53.376434]\n",
      " [61.64952 ]\n",
      " [36.82395 ]\n",
      " [41.65114 ]\n",
      " [41.28576 ]\n",
      " [72.326385]\n",
      " [97.46314 ]\n",
      " [60.71949 ]\n",
      " [54.56566 ]\n",
      " [36.151268]\n",
      " [57.40322 ]\n",
      " [56.38175 ]\n",
      " [44.241413]\n",
      " [35.23475 ]\n",
      " [46.89808 ]\n",
      " [56.22432 ]\n",
      " [97.45613 ]\n",
      " [42.790005]\n",
      " [83.93727 ]\n",
      " [84.40225 ]\n",
      " [71.73146 ]\n",
      " [67.898155]\n",
      " [50.89559 ]\n",
      " [35.124893]\n",
      " [61.19802 ]\n",
      " [55.342636]\n",
      " [33.922417]\n",
      " [30.079805]\n",
      " [44.081196]\n",
      " [37.677498]\n",
      " [46.762764]\n",
      " [68.16499 ]\n",
      " [93.99059 ]\n",
      " [75.2295  ]\n",
      " [59.53604 ]\n",
      " [40.04332 ]\n",
      " [57.818634]\n",
      " [44.93112 ]\n",
      " [35.098038]\n",
      " [72.6637  ]\n",
      " [64.19663 ]\n",
      " [43.946697]\n",
      " [37.095886]\n",
      " [67.17266 ]\n",
      " [58.462868]\n",
      " [31.694517]\n",
      " [44.536903]\n",
      " [43.19114 ]\n",
      " [33.801563]\n",
      " [33.740776]\n",
      " [90.70836 ]\n",
      " [50.187622]\n",
      " [54.3636  ]\n",
      " [52.48841 ]\n",
      " [33.878   ]\n",
      " [59.286102]\n",
      " [35.125793]\n",
      " [53.521767]\n",
      " [60.78228 ]\n",
      " [34.20781 ]\n",
      " [58.064873]\n",
      " [32.024128]\n",
      " [95.36566 ]\n",
      " [91.29137 ]\n",
      " [41.249405]\n",
      " [45.14825 ]\n",
      " [92.32437 ]\n",
      " [35.285553]\n",
      " [49.267204]\n",
      " [34.64713 ]\n",
      " [42.388733]\n",
      " [81.427666]\n",
      " [40.80944 ]\n",
      " [61.00644 ]\n",
      " [59.86902 ]\n",
      " [38.71192 ]\n",
      " [35.886314]\n",
      " [60.700935]\n",
      " [54.322968]\n",
      " [84.22688 ]\n",
      " [87.33544 ]\n",
      " [68.996346]\n",
      " [46.164185]\n",
      " [32.35932 ]\n",
      " [38.962112]\n",
      " [61.318016]\n",
      " [82.16058 ]\n",
      " [86.95875 ]\n",
      " [91.90898 ]\n",
      " [32.48242 ]\n",
      " [31.82616 ]\n",
      " [36.4367  ]\n",
      " [44.635853]\n",
      " [34.46439 ]\n",
      " [54.19564 ]\n",
      " [33.261375]\n",
      " [53.52564 ]\n",
      " [34.16823 ]\n",
      " [55.732117]\n",
      " [31.227644]\n",
      " [47.82316 ]\n",
      " [81.39424 ]\n",
      " [64.70007 ]\n",
      " [38.461803]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicciones: \", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 11s 40ms/step - loss: 18804.7031 - val_loss: 3189.3760\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 959.2726 - val_loss: 1725.3877\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 710.7464 - val_loss: 1170.5387\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 598.7290 - val_loss: 1054.8003\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 622.0212 - val_loss: 1776.8928\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 562.6777 - val_loss: 1132.8196\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 579.7819 - val_loss: 903.6609\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 662.1356 - val_loss: 887.8067\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 509.5499 - val_loss: 923.2798\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 454.6042 - val_loss: 1002.6008\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 477.8031 - val_loss: 745.0306\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 426.0778 - val_loss: 665.0641\n",
      "Epoch 13/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 449.0466 - val_loss: 662.6555\n",
      "Epoch 14/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 414.8853 - val_loss: 592.6712\n",
      "Epoch 15/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 430.3818 - val_loss: 569.0648\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 332.2954 - val_loss: 731.5226\n",
      "Epoch 17/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 375.8092 - val_loss: 779.7741\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 360.0378 - val_loss: 612.0689\n",
      "Epoch 19/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 359.3340 - val_loss: 514.4922\n",
      "Epoch 20/100\n",
      "71/71 [==============================] - 2s 25ms/step - loss: 349.2115 - val_loss: 545.6434\n",
      "Epoch 21/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 354.9420 - val_loss: 590.9167\n",
      "Epoch 22/100\n",
      "71/71 [==============================] - 2s 21ms/step - loss: 350.4250 - val_loss: 573.1986\n",
      "Epoch 23/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 301.5263 - val_loss: 550.1985\n",
      "Epoch 24/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 325.6403 - val_loss: 603.8784\n",
      "Epoch 25/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 310.2105 - val_loss: 466.3254\n",
      "Epoch 26/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 282.6766 - val_loss: 700.5533\n",
      "Epoch 27/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 340.6053 - val_loss: 620.0983\n",
      "Epoch 28/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 397.6683 - val_loss: 467.2274\n",
      "Epoch 29/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 290.2813 - val_loss: 441.6998\n",
      "Epoch 30/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 330.5227 - val_loss: 433.4464\n",
      "Epoch 31/100\n",
      "71/71 [==============================] - 2s 29ms/step - loss: 322.1111 - val_loss: 444.9237\n",
      "Epoch 32/100\n",
      "71/71 [==============================] - 2s 25ms/step - loss: 282.1337 - val_loss: 491.9740\n",
      "Epoch 33/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 293.4593 - val_loss: 445.6041\n",
      "Epoch 34/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 270.2110 - val_loss: 564.3998\n",
      "Epoch 35/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 286.2869 - val_loss: 417.8852\n",
      "Epoch 36/100\n",
      "71/71 [==============================] - 2s 25ms/step - loss: 281.7470 - val_loss: 601.7979\n",
      "Epoch 37/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 287.1679 - val_loss: 617.7509\n",
      "Epoch 38/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 290.5699 - val_loss: 403.5392\n",
      "Epoch 39/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 272.8308 - val_loss: 560.1520\n",
      "Epoch 40/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 269.6537 - val_loss: 417.5400\n",
      "Epoch 41/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 266.4138 - val_loss: 439.6348\n",
      "Epoch 42/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 259.1779 - val_loss: 545.7049\n",
      "Epoch 43/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 252.3676 - val_loss: 478.7316\n",
      "Epoch 44/100\n",
      "71/71 [==============================] - 2s 25ms/step - loss: 318.1533 - val_loss: 482.8279\n",
      "Epoch 45/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 291.8855 - val_loss: 408.2861\n",
      "Epoch 46/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 274.8059 - val_loss: 415.9157\n",
      "Epoch 47/100\n",
      "71/71 [==============================] - 2s 26ms/step - loss: 249.3756 - val_loss: 918.7590\n",
      "Epoch 48/100\n",
      "71/71 [==============================] - 2s 25ms/step - loss: 265.6403 - val_loss: 459.6610\n",
      "Epoch 49/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 267.4733 - val_loss: 414.9705\n",
      "Epoch 50/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 239.4590 - val_loss: 372.9834\n",
      "Epoch 51/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 250.1624 - val_loss: 390.2831\n",
      "Epoch 52/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 243.4294 - val_loss: 469.0155\n",
      "Epoch 53/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 254.7231 - val_loss: 405.6852\n",
      "Epoch 54/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 234.4680 - val_loss: 384.9668\n",
      "Epoch 55/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 233.0912 - val_loss: 472.6422\n",
      "Epoch 56/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 248.8796 - val_loss: 399.0455\n",
      "Epoch 57/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 237.7504 - val_loss: 500.4619\n",
      "Epoch 58/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 245.2961 - val_loss: 387.5799\n",
      "Epoch 59/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 256.4109 - val_loss: 438.0479\n",
      "Epoch 60/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 255.8515 - val_loss: 367.0201\n",
      "Epoch 61/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 263.8196 - val_loss: 512.5237\n",
      "Epoch 62/100\n",
      "71/71 [==============================] - 2s 25ms/step - loss: 242.8272 - val_loss: 407.9886\n",
      "Epoch 63/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 241.3877 - val_loss: 694.8213\n",
      "Epoch 64/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 280.5810 - val_loss: 388.0563\n",
      "Epoch 65/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 262.1731 - val_loss: 492.4458\n",
      "Epoch 66/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 244.4402 - val_loss: 383.5811\n",
      "Epoch 67/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 245.8197 - val_loss: 990.2513\n",
      "Epoch 68/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 327.6465 - val_loss: 405.1390\n",
      "Epoch 69/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 259.3404 - val_loss: 413.8889\n",
      "Epoch 70/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 251.7180 - val_loss: 555.0946\n",
      "Epoch 71/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 260.3931 - val_loss: 363.0848\n",
      "Epoch 72/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 235.7952 - val_loss: 365.0595\n",
      "Epoch 73/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 287.9768 - val_loss: 440.7952\n",
      "Epoch 74/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 245.7981 - val_loss: 465.9418\n",
      "Epoch 75/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 239.6628 - val_loss: 400.8905\n",
      "Epoch 76/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 241.5691 - val_loss: 366.5389\n",
      "Epoch 77/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 239.1620 - val_loss: 493.7249\n",
      "Epoch 78/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 230.6585 - val_loss: 480.6966\n",
      "Epoch 79/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 249.0209 - val_loss: 367.2136\n",
      "Epoch 80/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 239.1059 - val_loss: 369.8764\n",
      "Epoch 81/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 268.7968 - val_loss: 893.1068\n",
      "Epoch 82/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 280.9018 - val_loss: 401.8302\n",
      "Epoch 83/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 236.2493 - val_loss: 370.1496\n",
      "Epoch 84/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 258.3060 - val_loss: 380.1520\n",
      "Epoch 85/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 237.1697 - val_loss: 385.5449\n",
      "Epoch 86/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 234.9908 - val_loss: 437.1070\n",
      "Epoch 87/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 253.3878 - val_loss: 379.6325\n",
      "Epoch 88/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 238.4638 - val_loss: 463.6162\n",
      "Epoch 89/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 252.0010 - val_loss: 380.7679\n",
      "Epoch 90/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 228.4415 - val_loss: 467.5515\n",
      "Epoch 91/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 245.7707 - val_loss: 628.6061\n",
      "Epoch 92/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 255.1980 - val_loss: 373.5587\n",
      "Epoch 93/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 237.0770 - val_loss: 363.2078\n",
      "Epoch 94/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 225.9856 - val_loss: 397.6011\n",
      "Epoch 95/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 252.5605 - val_loss: 405.8180\n",
      "Epoch 96/100\n",
      "71/71 [==============================] - 2s 22ms/step - loss: 237.8928 - val_loss: 392.9638\n",
      "Epoch 97/100\n",
      "71/71 [==============================] - 2s 21ms/step - loss: 234.6306 - val_loss: 390.0059\n",
      "Epoch 98/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 232.9621 - val_loss: 443.3241\n",
      "Epoch 99/100\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 240.3857 - val_loss: 374.2239\n",
      "Epoch 100/100\n",
      "71/71 [==============================] - 2s 24ms/step - loss: 241.1626 - val_loss: 447.0701\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 447.0701\n",
      "P√©rdida en el conjunto de prueba: 447.0700988769531\n"
     ]
    }
   ],
   "source": [
    "# Cargar el conjunto de datos\n",
    "data = pd.read_csv('data.csv')  \n",
    "\n",
    "# Cambio de formato\n",
    "data['Year'] = data['Month'].apply(lambda x: int(x.split('-')[0]))\n",
    "data['Month'] = data['Month'].apply(lambda x: int(x.split('-')[1]))\n",
    "\n",
    "# Preparar los datos\n",
    "# Usando la secuencia como la cantidad de meses.\n",
    "sequence_length = 12\n",
    "input_data = []\n",
    "target_data = []\n",
    "for i in range(len(data) - sequence_length):\n",
    "    input_sequence = data[['Year', 'Month', 'Sunspots']].iloc[i : i + sequence_length].values\n",
    "    target_value = data['Sunspots'].iloc[i + sequence_length]\n",
    "    input_data.append(input_sequence)\n",
    "    target_data.append(target_value)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "target_data = np.array(target_data)\n",
    "\n",
    "# Dividiendo el conjunto de datos en entrenamiento y prueba.\n",
    "train_size = int(0.8 * len(input_data))\n",
    "X_train, X_test = input_data[:train_size], input_data[train_size:]\n",
    "y_train, y_test = target_data[:train_size], target_data[train_size:]\n",
    "\n",
    "# Creando la estructura de la RNN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(sequence_length, 3)),  # Entrada con 3 caracter√≠sticas (Year, Month, Sunspots)\n",
    "    tf.keras.layers.SimpleRNN(64, activation='relu', return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(64, activation='relu', return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(32, activation='relu', return_sequences=True),\n",
    "    tf.keras.layers.Flatten(),  # Aplananando las secuencias para la capa densa\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Capa de salida para la regresi√≥n\n",
    "])\n",
    "\n",
    "# Compilando el modelo.\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenando el modelo.\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluando el modelo.\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"P√©rdida en el conjunto de prueba:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resumen del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_4 (SimpleRNN)    (None, 12, 64)            4352      \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 12, 64)            8256      \n",
      "                                                                 \n",
      " simple_rnn_6 (SimpleRNN)    (None, 12, 32)            3104      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 32)                12320     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28065 (109.63 KB)\n",
      "Trainable params: 28065 (109.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Resumen del modelo para verificar la arquitectura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 19s 45ms/step - loss: 4529.8960 - val_loss: 4279.0942\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 3405.8191 - val_loss: 1992.4381\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1865.4258 - val_loss: 1836.3619\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1817.5349 - val_loss: 1823.4009\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1806.1677 - val_loss: 1806.2855\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1781.7252 - val_loss: 1785.8746\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1771.1294 - val_loss: 1777.2109\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1770.3401 - val_loss: 1769.5818\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1761.2969 - val_loss: 1769.9994\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1758.7489 - val_loss: 1781.4415\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1756.4423 - val_loss: 1774.4521\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1760.0796 - val_loss: 1779.2659\n",
      "Epoch 13/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1755.6676 - val_loss: 1760.7668\n",
      "Epoch 14/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1749.0110 - val_loss: 1765.7068\n",
      "Epoch 15/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1752.3855 - val_loss: 1755.0122\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1743.7162 - val_loss: 1757.4119\n",
      "Epoch 17/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1746.2035 - val_loss: 1761.4133\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1745.6991 - val_loss: 1767.4141\n",
      "Epoch 19/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1745.8689 - val_loss: 1756.9625\n",
      "Epoch 20/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1742.0204 - val_loss: 1774.3285\n",
      "Epoch 21/100\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 1741.0845 - val_loss: 1755.0292\n",
      "Epoch 22/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1745.6124 - val_loss: 1764.8523\n",
      "Epoch 23/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1737.9729 - val_loss: 1757.5310\n",
      "Epoch 24/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1734.6969 - val_loss: 1751.4442\n",
      "Epoch 25/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1739.6736 - val_loss: 1760.2858\n",
      "Epoch 26/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1738.8395 - val_loss: 1748.8754\n",
      "Epoch 27/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1736.9009 - val_loss: 1777.6710\n",
      "Epoch 28/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1740.6228 - val_loss: 1765.0984\n",
      "Epoch 29/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1737.1174 - val_loss: 1756.5511\n",
      "Epoch 30/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1738.8300 - val_loss: 1762.8119\n",
      "Epoch 31/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1740.4393 - val_loss: 1747.1912\n",
      "Epoch 32/100\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 1736.7682 - val_loss: 1746.7579\n",
      "Epoch 33/100\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 1737.4352 - val_loss: 1777.6353\n",
      "Epoch 34/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1731.7975 - val_loss: 1787.9115\n",
      "Epoch 35/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1735.8595 - val_loss: 1777.4347\n",
      "Epoch 36/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1732.1604 - val_loss: 1751.1797\n",
      "Epoch 37/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1730.0968 - val_loss: 1747.1941\n",
      "Epoch 38/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1731.0491 - val_loss: 1751.1057\n",
      "Epoch 39/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1729.5809 - val_loss: 1748.3894\n",
      "Epoch 40/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1728.7473 - val_loss: 1744.7119\n",
      "Epoch 41/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1727.5594 - val_loss: 1755.4403\n",
      "Epoch 42/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1728.5115 - val_loss: 1751.3218\n",
      "Epoch 43/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1725.7290 - val_loss: 1757.5442\n",
      "Epoch 44/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1731.5150 - val_loss: 1757.6809\n",
      "Epoch 45/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1726.9012 - val_loss: 1751.9497\n",
      "Epoch 46/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1729.4457 - val_loss: 1752.1639\n",
      "Epoch 47/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1725.7588 - val_loss: 1755.4395\n",
      "Epoch 48/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1725.0081 - val_loss: 1750.3666\n",
      "Epoch 49/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1727.5635 - val_loss: 1759.4106\n",
      "Epoch 50/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1720.4318 - val_loss: 1754.3313\n",
      "Epoch 51/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1722.2853 - val_loss: 1745.7329\n",
      "Epoch 52/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1725.2687 - val_loss: 1747.2402\n",
      "Epoch 53/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1727.2435 - val_loss: 1745.4874\n",
      "Epoch 54/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1723.6168 - val_loss: 1749.5258\n",
      "Epoch 55/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1721.9930 - val_loss: 1748.4897\n",
      "Epoch 56/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1719.6847 - val_loss: 1774.9415\n",
      "Epoch 57/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1721.4180 - val_loss: 1773.2705\n",
      "Epoch 58/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1715.7843 - val_loss: 1746.2114\n",
      "Epoch 59/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1722.2351 - val_loss: 1767.7849\n",
      "Epoch 60/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1724.1495 - val_loss: 1748.0332\n",
      "Epoch 61/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1722.0675 - val_loss: 1780.9867\n",
      "Epoch 62/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1730.9034 - val_loss: 1760.4844\n",
      "Epoch 63/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1718.5380 - val_loss: 1748.2546\n",
      "Epoch 64/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1723.4231 - val_loss: 1746.3774\n",
      "Epoch 65/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1715.6791 - val_loss: 1741.9946\n",
      "Epoch 66/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1717.4525 - val_loss: 1747.2368\n",
      "Epoch 67/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1716.5972 - val_loss: 1745.7711\n",
      "Epoch 68/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1732.9742 - val_loss: 1743.4771\n",
      "Epoch 69/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1715.1526 - val_loss: 1742.6853\n",
      "Epoch 70/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1715.7375 - val_loss: 1749.3361\n",
      "Epoch 71/100\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 1711.1938 - val_loss: 1781.2186\n",
      "Epoch 72/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1718.0139 - val_loss: 1772.1224\n",
      "Epoch 73/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1711.7729 - val_loss: 1755.3489\n",
      "Epoch 74/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1715.0273 - val_loss: 1748.0051\n",
      "Epoch 75/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1717.2788 - val_loss: 1743.0276\n",
      "Epoch 76/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1710.5457 - val_loss: 1744.6849\n",
      "Epoch 77/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1714.6631 - val_loss: 1755.6396\n",
      "Epoch 78/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1713.3953 - val_loss: 1748.4475\n",
      "Epoch 79/100\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 1711.9026 - val_loss: 1749.1256\n",
      "Epoch 80/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1719.7563 - val_loss: 1742.4215\n",
      "Epoch 81/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1709.5201 - val_loss: 1745.3119\n",
      "Epoch 82/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1709.7952 - val_loss: 1748.8995\n",
      "Epoch 83/100\n",
      "71/71 [==============================] - 1s 16ms/step - loss: 1707.6011 - val_loss: 1782.1764\n",
      "Epoch 84/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1708.2485 - val_loss: 1756.1290\n",
      "Epoch 85/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1706.9260 - val_loss: 1755.8615\n",
      "Epoch 86/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1704.5525 - val_loss: 1743.7206\n",
      "Epoch 87/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1708.2094 - val_loss: 1754.3979\n",
      "Epoch 88/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1711.0433 - val_loss: 1745.0253\n",
      "Epoch 89/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1704.4675 - val_loss: 1772.2208\n",
      "Epoch 90/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1710.8341 - val_loss: 1758.9108\n",
      "Epoch 91/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1705.6448 - val_loss: 1753.8271\n",
      "Epoch 92/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1719.6237 - val_loss: 1746.9377\n",
      "Epoch 93/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1704.8463 - val_loss: 1748.7780\n",
      "Epoch 94/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1704.0723 - val_loss: 1737.4557\n",
      "Epoch 95/100\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 1705.7161 - val_loss: 1739.2938\n",
      "Epoch 96/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1702.7473 - val_loss: 1744.8191\n",
      "Epoch 97/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1700.6891 - val_loss: 1739.6709\n",
      "Epoch 98/100\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 1699.3672 - val_loss: 1752.3544\n",
      "Epoch 99/100\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 1702.7740 - val_loss: 1754.2473\n",
      "Epoch 100/100\n",
      "71/71 [==============================] - 1s 13ms/step - loss: 1701.5571 - val_loss: 1743.1844\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1743.1844\n",
      "P√©rdida en el conjunto de prueba: 1743.1844482421875\n"
     ]
    }
   ],
   "source": [
    "# Cargando el conjunto de datos\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Haciendo unos cambios al formato.\n",
    "data['Year'] = data['Month'].apply(lambda x: int(x.split('-')[0]))\n",
    "data['Month'] = data['Month'].apply(lambda x: int(x.split('-')[1]))\n",
    "\n",
    "# Dividiendo los datos en caracter√≠sticas (X) y etiquetas (y)\n",
    "X = data[['Year', 'Month']]\n",
    "y = data['Sunspots']\n",
    "\n",
    "# Dividiendo los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalando los datos para mejorar el rendimiento de la red neuronal\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Haciendo reshape para que los datos sean adecuados para la capa LSTM (samples, timesteps, features)\n",
    "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "\n",
    "# Creando la estructura de la red neuronal con LSTM\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, activation='relu', input_shape=(1, 2), return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32, activation='relu', return_sequences=True),\n",
    "    tf.keras.layers.LSTM(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Capa de salida para la regresi√≥n\n",
    "])\n",
    "\n",
    "# Compilando\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenando el modelo\n",
    "model.fit(X_train_lstm, y_train, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test))\n",
    "\n",
    "# Evaluando el modelo\n",
    "loss = model.evaluate(X_test_lstm, y_test)\n",
    "print(\"P√©rdida en el conjunto de prueba:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 5ms/step\n",
      "Predicciones:  [[47.257523]\n",
      " [59.160065]\n",
      " [43.008945]\n",
      " [47.005684]\n",
      " [86.18191 ]\n",
      " [54.394848]\n",
      " [42.00695 ]\n",
      " [94.16267 ]\n",
      " [55.20066 ]\n",
      " [43.403862]\n",
      " [46.41416 ]\n",
      " [43.48442 ]\n",
      " [46.21351 ]\n",
      " [52.174793]\n",
      " [43.546906]\n",
      " [54.71952 ]\n",
      " [86.618034]\n",
      " [63.575783]\n",
      " [55.191017]\n",
      " [45.406155]\n",
      " [79.13219 ]\n",
      " [48.266373]\n",
      " [51.946365]\n",
      " [45.173454]\n",
      " [38.150295]\n",
      " [52.643127]\n",
      " [37.937893]\n",
      " [47.46921 ]\n",
      " [47.55989 ]\n",
      " [43.23356 ]\n",
      " [47.50571 ]\n",
      " [44.71327 ]\n",
      " [52.5412  ]\n",
      " [44.170433]\n",
      " [44.903618]\n",
      " [41.281063]\n",
      " [42.507744]\n",
      " [46.202854]\n",
      " [47.318157]\n",
      " [46.34796 ]\n",
      " [47.42233 ]\n",
      " [55.84536 ]\n",
      " [76.878914]\n",
      " [59.26564 ]\n",
      " [49.051907]\n",
      " [54.831387]\n",
      " [52.50445 ]\n",
      " [36.834866]\n",
      " [43.745327]\n",
      " [47.42467 ]\n",
      " [44.47827 ]\n",
      " [48.435764]\n",
      " [46.94841 ]\n",
      " [47.615013]\n",
      " [83.65575 ]\n",
      " [40.390736]\n",
      " [43.83707 ]\n",
      " [42.989273]\n",
      " [93.25926 ]\n",
      " [64.82509 ]\n",
      " [47.319942]\n",
      " [46.37905 ]\n",
      " [60.27206 ]\n",
      " [49.65989 ]\n",
      " [86.84937 ]\n",
      " [52.545498]\n",
      " [89.5581  ]\n",
      " [67.88829 ]\n",
      " [38.965336]\n",
      " [45.471375]\n",
      " [45.779797]\n",
      " [44.074074]\n",
      " [49.102074]\n",
      " [46.926846]\n",
      " [48.18541 ]\n",
      " [43.12845 ]\n",
      " [43.521027]\n",
      " [37.602024]\n",
      " [70.89517 ]\n",
      " [45.09229 ]\n",
      " [38.309097]\n",
      " [63.62942 ]\n",
      " [87.27828 ]\n",
      " [55.732162]\n",
      " [45.84996 ]\n",
      " [55.118423]\n",
      " [51.02734 ]\n",
      " [72.01217 ]\n",
      " [48.79017 ]\n",
      " [91.54519 ]\n",
      " [46.69549 ]\n",
      " [38.97706 ]\n",
      " [46.7826  ]\n",
      " [46.831352]\n",
      " [46.561478]\n",
      " [44.670498]\n",
      " [66.09704 ]\n",
      " [52.43743 ]\n",
      " [47.660892]\n",
      " [38.555077]\n",
      " [45.629704]\n",
      " [46.217373]\n",
      " [43.39541 ]\n",
      " [50.690525]\n",
      " [55.56404 ]\n",
      " [46.851486]\n",
      " [46.621387]\n",
      " [41.041805]\n",
      " [81.3771  ]\n",
      " [47.55278 ]\n",
      " [49.53038 ]\n",
      " [82.396194]\n",
      " [46.438103]\n",
      " [49.248783]\n",
      " [47.275257]\n",
      " [45.247593]\n",
      " [83.715775]\n",
      " [41.303276]\n",
      " [81.49457 ]\n",
      " [55.178326]\n",
      " [43.574192]\n",
      " [45.553642]\n",
      " [44.525562]\n",
      " [45.51428 ]\n",
      " [38.771904]\n",
      " [45.422478]\n",
      " [44.805153]\n",
      " [87.75005 ]\n",
      " [47.59015 ]\n",
      " [53.312305]\n",
      " [38.641087]\n",
      " [81.884964]\n",
      " [52.972687]\n",
      " [46.69817 ]\n",
      " [51.55661 ]\n",
      " [46.601864]\n",
      " [42.207493]\n",
      " [41.97325 ]\n",
      " [36.089146]\n",
      " [52.956367]\n",
      " [46.648853]\n",
      " [53.792896]\n",
      " [35.023823]\n",
      " [46.16828 ]\n",
      " [55.830925]\n",
      " [46.69929 ]\n",
      " [47.274876]\n",
      " [47.290527]\n",
      " [71.070076]\n",
      " [55.218895]\n",
      " [47.47301 ]\n",
      " [80.89432 ]\n",
      " [58.644245]\n",
      " [45.3876  ]\n",
      " [49.3355  ]\n",
      " [46.874557]\n",
      " [54.41594 ]\n",
      " [53.92556 ]\n",
      " [48.64646 ]\n",
      " [64.535324]\n",
      " [90.56153 ]\n",
      " [45.997704]\n",
      " [53.287415]\n",
      " [44.88262 ]\n",
      " [47.766953]\n",
      " [64.74286 ]\n",
      " [47.567146]\n",
      " [76.52246 ]\n",
      " [42.85338 ]\n",
      " [38.50822 ]\n",
      " [44.15473 ]\n",
      " [44.621124]\n",
      " [46.137238]\n",
      " [55.60702 ]\n",
      " [45.321827]\n",
      " [50.72931 ]\n",
      " [45.877064]\n",
      " [41.032604]\n",
      " [46.354385]\n",
      " [45.29212 ]\n",
      " [46.67458 ]\n",
      " [47.43345 ]\n",
      " [45.00366 ]\n",
      " [55.355076]\n",
      " [44.489628]\n",
      " [53.80672 ]\n",
      " [47.457016]\n",
      " [46.03824 ]\n",
      " [52.677757]\n",
      " [46.549255]\n",
      " [46.225296]\n",
      " [37.812786]\n",
      " [46.39473 ]\n",
      " [44.130867]\n",
      " [44.338493]\n",
      " [40.84469 ]\n",
      " [47.418007]\n",
      " [46.950867]\n",
      " [71.29738 ]\n",
      " [45.631134]\n",
      " [47.129814]\n",
      " [62.881413]\n",
      " [47.498795]\n",
      " [78.85074 ]\n",
      " [54.789917]\n",
      " [46.385773]\n",
      " [46.868473]\n",
      " [45.29536 ]\n",
      " [51.4106  ]\n",
      " [42.882057]\n",
      " [42.169235]\n",
      " [42.00169 ]\n",
      " [46.90781 ]\n",
      " [44.415787]\n",
      " [72.94456 ]\n",
      " [44.973824]\n",
      " [55.84861 ]\n",
      " [46.19212 ]\n",
      " [51.42134 ]\n",
      " [38.06546 ]\n",
      " [45.831383]\n",
      " [55.478905]\n",
      " [48.711544]\n",
      " [45.31235 ]\n",
      " [49.696922]\n",
      " [67.0446  ]\n",
      " [39.112083]\n",
      " [44.466652]\n",
      " [54.36091 ]\n",
      " [47.312855]\n",
      " [76.218254]\n",
      " [51.053825]\n",
      " [60.42974 ]\n",
      " [60.700233]\n",
      " [42.58099 ]\n",
      " [47.58679 ]\n",
      " [45.874996]\n",
      " [50.237843]\n",
      " [49.42659 ]\n",
      " [53.496754]\n",
      " [38.38844 ]\n",
      " [37.60964 ]\n",
      " [44.78411 ]\n",
      " [49.898647]\n",
      " [45.09458 ]\n",
      " [40.20618 ]\n",
      " [44.744617]\n",
      " [49.69216 ]\n",
      " [69.58611 ]\n",
      " [40.59646 ]\n",
      " [45.60842 ]\n",
      " [45.085217]\n",
      " [79.72939 ]\n",
      " [47.373077]\n",
      " [63.550716]\n",
      " [44.25789 ]\n",
      " [41.385967]\n",
      " [85.950966]\n",
      " [44.21801 ]\n",
      " [47.324905]\n",
      " [46.535694]\n",
      " [35.21527 ]\n",
      " [52.359993]\n",
      " [72.14652 ]\n",
      " [66.68367 ]\n",
      " [61.500908]\n",
      " [42.49303 ]\n",
      " [48.079254]\n",
      " [91.94516 ]\n",
      " [61.919865]\n",
      " [46.815086]\n",
      " [35.976616]\n",
      " [47.65975 ]\n",
      " [48.075546]\n",
      " [57.539658]\n",
      " [45.81113 ]\n",
      " [43.804832]\n",
      " [39.628708]\n",
      " [51.308964]\n",
      " [44.740463]\n",
      " [44.42891 ]\n",
      " [39.833515]\n",
      " [47.4035  ]\n",
      " [45.712288]\n",
      " [54.790028]\n",
      " [58.948036]\n",
      " [40.874226]\n",
      " [39.632748]\n",
      " [50.993824]\n",
      " [43.343163]\n",
      " [45.136337]\n",
      " [53.913868]\n",
      " [52.233963]\n",
      " [49.60103 ]\n",
      " [55.375977]\n",
      " [53.54594 ]\n",
      " [47.050404]\n",
      " [83.054756]\n",
      " [51.875103]\n",
      " [51.45588 ]\n",
      " [44.646763]\n",
      " [46.180885]\n",
      " [59.71833 ]\n",
      " [59.5038  ]\n",
      " [47.40548 ]\n",
      " [59.883484]\n",
      " [53.418423]\n",
      " [48.53755 ]\n",
      " [47.43637 ]\n",
      " [49.998234]\n",
      " [47.155132]\n",
      " [37.53241 ]\n",
      " [45.19638 ]\n",
      " [52.649975]\n",
      " [47.175705]\n",
      " [46.950882]\n",
      " [46.288498]\n",
      " [51.293518]\n",
      " [37.969414]\n",
      " [41.13632 ]\n",
      " [51.62961 ]\n",
      " [61.860798]\n",
      " [43.29945 ]\n",
      " [41.410202]\n",
      " [52.52083 ]\n",
      " [41.548325]\n",
      " [47.219227]\n",
      " [43.51397 ]\n",
      " [53.131943]\n",
      " [46.133167]\n",
      " [44.00171 ]\n",
      " [47.041954]\n",
      " [46.877228]\n",
      " [48.908726]\n",
      " [72.24599 ]\n",
      " [63.86242 ]\n",
      " [50.48325 ]\n",
      " [54.372948]\n",
      " [46.600845]\n",
      " [52.78689 ]\n",
      " [52.062202]\n",
      " [49.557735]\n",
      " [73.636955]\n",
      " [61.0114  ]\n",
      " [96.84416 ]\n",
      " [44.64855 ]\n",
      " [44.061287]\n",
      " [47.066628]\n",
      " [69.37479 ]\n",
      " [52.02853 ]\n",
      " [40.820515]\n",
      " [41.878716]\n",
      " [86.56329 ]\n",
      " [47.465073]\n",
      " [72.84919 ]\n",
      " [51.18315 ]\n",
      " [47.754734]\n",
      " [86.495544]\n",
      " [89.76852 ]\n",
      " [52.410748]\n",
      " [48.997078]\n",
      " [72.5454  ]\n",
      " [46.278862]\n",
      " [51.05087 ]\n",
      " [65.1965  ]\n",
      " [86.36563 ]\n",
      " [47.498463]\n",
      " [39.830246]\n",
      " [53.018112]\n",
      " [45.425293]\n",
      " [44.51365 ]\n",
      " [45.143887]\n",
      " [50.84422 ]\n",
      " [64.55119 ]\n",
      " [44.590443]\n",
      " [46.118973]\n",
      " [45.97972 ]\n",
      " [82.38097 ]\n",
      " [47.204597]\n",
      " [47.69848 ]\n",
      " [47.70339 ]\n",
      " [45.862926]\n",
      " [47.269577]\n",
      " [40.14062 ]\n",
      " [52.94774 ]\n",
      " [37.301544]\n",
      " [38.059723]\n",
      " [50.37204 ]\n",
      " [47.940765]\n",
      " [46.09278 ]\n",
      " [86.994705]\n",
      " [43.12762 ]\n",
      " [48.38023 ]\n",
      " [45.96699 ]\n",
      " [46.408375]\n",
      " [52.921764]\n",
      " [47.527096]\n",
      " [47.640045]\n",
      " [47.28151 ]\n",
      " [49.305626]\n",
      " [51.888325]\n",
      " [90.29347 ]\n",
      " [46.88112 ]\n",
      " [46.395504]\n",
      " [87.54289 ]\n",
      " [49.114277]\n",
      " [71.0573  ]\n",
      " [46.92952 ]\n",
      " [43.81821 ]\n",
      " [50.648502]\n",
      " [53.480362]\n",
      " [47.191044]\n",
      " [46.681786]\n",
      " [50.82137 ]\n",
      " [52.42636 ]\n",
      " [56.23863 ]\n",
      " [54.378944]\n",
      " [44.483986]\n",
      " [46.54403 ]\n",
      " [39.947655]\n",
      " [42.09526 ]\n",
      " [45.17104 ]\n",
      " [53.018547]\n",
      " [47.336876]\n",
      " [43.68675 ]\n",
      " [47.52558 ]\n",
      " [47.69989 ]\n",
      " [67.55131 ]\n",
      " [67.111   ]\n",
      " [91.17391 ]\n",
      " [94.65759 ]\n",
      " [37.38156 ]\n",
      " [47.72133 ]\n",
      " [54.949196]\n",
      " [38.723312]\n",
      " [40.18561 ]\n",
      " [45.705242]\n",
      " [43.215073]\n",
      " [48.50055 ]\n",
      " [56.763405]\n",
      " [62.71446 ]\n",
      " [39.34552 ]\n",
      " [45.983047]\n",
      " [43.966507]\n",
      " [54.161465]\n",
      " [51.836983]\n",
      " [46.920456]\n",
      " [54.25408 ]\n",
      " [53.2868  ]\n",
      " [53.911858]\n",
      " [44.068535]\n",
      " [46.784332]\n",
      " [45.21025 ]\n",
      " [81.470055]\n",
      " [48.31529 ]\n",
      " [46.01189 ]\n",
      " [47.89585 ]\n",
      " [90.02956 ]\n",
      " [48.149307]\n",
      " [77.974754]\n",
      " [46.72208 ]\n",
      " [47.404507]\n",
      " [45.47612 ]\n",
      " [47.23572 ]\n",
      " [44.78296 ]\n",
      " [70.07471 ]\n",
      " [96.11819 ]\n",
      " [46.965157]\n",
      " [53.603424]\n",
      " [37.524704]\n",
      " [47.75767 ]\n",
      " [47.421314]\n",
      " [49.146873]\n",
      " [43.541283]\n",
      " [50.54986 ]\n",
      " [46.70452 ]\n",
      " [97.03616 ]\n",
      " [47.698486]\n",
      " [82.24453 ]\n",
      " [83.72911 ]\n",
      " [70.71542 ]\n",
      " [66.1698  ]\n",
      " [51.646114]\n",
      " [44.54792 ]\n",
      " [47.363594]\n",
      " [46.7149  ]\n",
      " [43.37583 ]\n",
      " [44.713024]\n",
      " [45.753242]\n",
      " [43.6416  ]\n",
      " [46.05172 ]\n",
      " [65.25138 ]\n",
      " [92.73918 ]\n",
      " [72.830414]\n",
      " [47.39947 ]\n",
      " [43.61047 ]\n",
      " [56.859016]\n",
      " [44.79117 ]\n",
      " [40.50508 ]\n",
      " [69.53332 ]\n",
      " [62.053352]\n",
      " [43.780113]\n",
      " [46.44159 ]\n",
      " [66.2191  ]\n",
      " [46.89159 ]\n",
      " [43.213486]\n",
      " [44.64261 ]\n",
      " [47.907547]\n",
      " [45.20449 ]\n",
      " [44.90606 ]\n",
      " [90.42707 ]\n",
      " [52.47629 ]\n",
      " [53.12503 ]\n",
      " [52.607246]\n",
      " [43.924488]\n",
      " [56.4291  ]\n",
      " [36.974804]\n",
      " [46.461132]\n",
      " [60.089577]\n",
      " [45.36144 ]\n",
      " [47.80013 ]\n",
      " [43.848297]\n",
      " [93.20613 ]\n",
      " [89.89171 ]\n",
      " [48.88301 ]\n",
      " [48.83152 ]\n",
      " [90.233894]\n",
      " [42.21291 ]\n",
      " [51.61023 ]\n",
      " [45.604786]\n",
      " [47.485943]\n",
      " [79.128716]\n",
      " [45.862347]\n",
      " [47.63687 ]\n",
      " [59.23141 ]\n",
      " [46.898087]\n",
      " [37.691463]\n",
      " [46.917664]\n",
      " [53.18094 ]\n",
      " [81.874695]\n",
      " [85.29174 ]\n",
      " [68.00668 ]\n",
      " [46.053825]\n",
      " [44.58996 ]\n",
      " [40.082447]\n",
      " [47.60808 ]\n",
      " [80.84679 ]\n",
      " [85.36549 ]\n",
      " [90.03441 ]\n",
      " [43.66873 ]\n",
      " [40.561977]\n",
      " [44.26361 ]\n",
      " [48.960518]\n",
      " [39.913265]\n",
      " [53.66101 ]\n",
      " [43.321236]\n",
      " [52.616676]\n",
      " [37.971775]\n",
      " [53.96507 ]\n",
      " [35.446438]\n",
      " [51.570442]\n",
      " [79.6255  ]\n",
      " [61.24293 ]\n",
      " [45.071358]]\n"
     ]
    }
   ],
   "source": [
    "# Realizando predicciones\n",
    "predictions = model.predict(X_test_lstm)\n",
    "print(\"Predicciones: \", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
